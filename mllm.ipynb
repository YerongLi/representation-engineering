{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75644780-ff3e-47ac-a5f3-50158c4a259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type internlmxcomposer2 to instantiate a model of type internlm. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set max length to 4096\n",
      "Position interpolate from 24x24 to 35x35\n",
      "Set max length to 4096\n",
      "Position interpolate from 24x24 to 35x35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.69s/it]\n",
      "Some weights of ContrastVecInternlmForCausalLM were not initialized from the model checkpoint at /home/yerong2/models/internlm-xcomposer2-vl-7b and are newly initialized: ['vit.vision_tower.vision_model.post_layernorm.bias', 'vit.vision_tower.vision_model.post_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering constrastive greedy search\n",
      "In the image, a majestic tiger is the focal point, lying comfortably on a lush, green grassy field. The tiger's body is oriented towards the left side of the image, but its head is turned to face the camera, giving us a direct gaze. The tiger's fur is a striking mix of orange and black stripes, with a white underbelly that contrasts with the greenery beneath it. The background is a blurred expanse of green, likely a mix of trees and bushes, which further emphasizes the tiger as the main subject of this image. The tiger's relaxed posture and the serene environment create a sense of tranquility.[UNUSED_TOKEN_145]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from repe.mllm_utils import auto_configure_device_map\n",
    "from repe.mllm_rep_control_contrast_vec import ContrastVecInternlmForCausalLM\n",
    "from repe.modeling_internlm_xcomposer2 import InternLMXComposer2ForCausalLM\n",
    "\n",
    "model_name_or_path = \"/home/yerong2/models/internlm-xcomposer2-vl-7b\"\n",
    "\n",
    "model = ContrastVecInternlmForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).eval()\n",
    "# model = InternLMXComposer2ForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).eval()\n",
    "\n",
    "from accelerate import dispatch_model\n",
    "device_map = auto_configure_device_map(2)\n",
    "model = dispatch_model(model, device_map=device_map)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "# query = '<ImageHere>Please describe this image'\n",
    "# image = ['../tiger.jpeg']\n",
    "image = 'tiger.jpeg'\n",
    "\n",
    "questions = ['<ImageHere>Please describe this image']\n",
    "query = questions[0]\n",
    "\n",
    "pos_p = 'Give a truthful answer. '\n",
    "neg_p = 'Give an untruthful answer. '\n",
    "\n",
    "def Encode(model, query, image):\n",
    "    meta_instruction = 'You are an AI assistant whose name is InternLM-XComposer (浦语·灵笔).\\n'\n",
    "    '- InternLM-XComposer (浦语·灵笔) is a multi-modality conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.\\n'\n",
    "    '- InternLM-XComposer (浦语·灵笔) can understand and communicate fluently in the language chosen by the user such as English and 中文.\\n'\n",
    "    '- InternLM-XComposer (浦语·灵笔) is capable of comprehending and articulating responses effectively based on the provided image.'\n",
    "    \n",
    "    image = model.encode_img(image)\n",
    "        \n",
    "    inputs, im_mask = model.interleav_wrap_chat(model.tokenizer, query, image, [], meta_instruction)\n",
    "    # print(model.tokenizer is None) # DEBUG\n",
    "    inputs = {\n",
    "        k: v.to(model.device)\n",
    "        for k, v in inputs.items() if torch.is_tensor(v)\n",
    "    }    \n",
    "    return inputs, im_mask\n",
    "\n",
    "contrast_tokens=-8 # last {tokens} tokens are used to compute the diff in hidden_states\n",
    "alpha = 0\n",
    "layer_ids = np.arange(0, 32, 2).tolist()\n",
    "# also add end-of-assistant token in eos token id to avoid unnecessary generation\n",
    "eos_token_id = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(['[UNUSED_TOKEN_145]'])[0]\n",
    "]\n",
    "model.eval()\n",
    "\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "\n",
    "    for q in questions:\n",
    "        q_pos = pos_p + q\n",
    "        q_neg = neg_p + q\n",
    "        \n",
    "        # enc = tokenizer([input, input_pos, input_neg], return_tensors='pt', padding='longest').to(model.device)\n",
    "        inputs, im_mask = Encode(model, q, image)\n",
    "        pos_inputs, pos_im_mask = Encode(model, q_pos, image)\n",
    "        neg_inputs, neg_im_mask = Encode(model, q_neg, image)\n",
    "\n",
    "        \n",
    "        repe_args = dict(pos_input_ids=pos_inputs,\n",
    "                 pos_img_mask=pos_im_mask,\n",
    "                 neg_input_ids=neg_inputs,\n",
    "                 neg_img_mask=neg_im_mask,\n",
    "                 contrast_tokens=contrast_tokens,\n",
    "                 compute_contrast=True,\n",
    "                 use_cache=False, # not yet supporting generation with use_cache         \n",
    "                 alpha=alpha,\n",
    "                 control_layer_ids=layer_ids)\n",
    "        \n",
    "        \n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            eos_token_id=eos_token_id,\n",
    "            im_mask=im_mask,\n",
    "            do_sample=False,\n",
    "            # **repe_args,\n",
    "        )\n",
    "        \n",
    "        # @@@ controlled_outputs = model.generate(input_ids, \n",
    "        #                          attention_mask=attention_mask, \n",
    "        #                          max_new_tokens=256, \n",
    "        #                          do_sample=False, \n",
    "        #                          use_cache=False, # not yet supporting generation with use_cache\n",
    "        #                          **repe_args)\n",
    "        if image is None:\n",
    "            outputs = outputs[0].cpu().tolist()[len(inputs['input_ids'][0]):]\n",
    "        else:\n",
    "            outputs = outputs[0].cpu().tolist()\n",
    "        response = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f8dde1-0f22-4067-8c0b-52d46461f396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/yerong2/local/miniconda3/envs/mllm/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "You are using a model of type internlmxcomposer2 to instantiate a model of type internlm. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set max length to 4096\n",
      "Position interpolate from 24x24 to 35x35\n",
      "Set max length to 4096\n",
      "Position interpolate from 24x24 to 35x35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.63s/it]\n",
      "Some weights of ContrastVecInternlmForCausalLM were not initialized from the model checkpoint at /home/yerong2/models/internlm-xcomposer2-vl-7b and are newly initialized: ['vit.vision_tower.vision_model.post_layernorm.bias', 'vit.vision_tower.vision_model.post_layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the image, a majestic tiger is the focal point, lying comfortably on a lush, green grassy field. The tiger's body is oriented towards the left side of the image, but its head is turned to face the camera, giving us a direct gaze. The tiger's fur is a striking mix of orange and black stripes, with a white underbelly that contrasts with the greenery beneath it. The background is a blurred expanse of green, likely a mix of trees and bushes, which further emphasizes the tiger as the main subject of this image. The tiger's relaxed posture and the serene environment create a sense of tranquility.[UNUSED_TOKEN_145]\n"
     ]
    }
   ],
   "source": [
    "# Backup\n",
    "# Backup\n",
    "# Backup\n",
    "# Backup\n",
    "# Backup\n",
    "# Backup\n",
    "# Backup\n",
    "# Backup\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, pipeline, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from repe.mllm_utils import auto_configure_device_map\n",
    "from repe.mllm_rep_control_contrast_vec import ContrastVecInternlmForCausalLM\n",
    "from repe.modeling_internlm_xcomposer2 import InternLMXComposer2ForCausalLM\n",
    "\n",
    "model_name_or_path = \"/home/yerong2/models/internlm-xcomposer2-vl-7b\"\n",
    "\n",
    "model = ContrastVecInternlmForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).eval()\n",
    "# model = InternLMXComposer2ForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True).eval()\n",
    "\n",
    "from accelerate import dispatch_model\n",
    "device_map = auto_configure_device_map(2)\n",
    "model = dispatch_model(model, device_map=device_map)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "# query = '<ImageHere>Please describe this image'\n",
    "# image = ['../tiger.jpeg']\n",
    "image = 'tiger.jpeg'\n",
    "\n",
    "questions = ['<ImageHere>Please describe this image']\n",
    "query = questions[0]\n",
    "\n",
    "pos_p = 'Give a truthful answer. '\n",
    "neg_p = 'Give an untruthful answer. '\n",
    "\n",
    "def Encode(model, query, image):\n",
    "    meta_instruction = 'You are an AI assistant whose name is InternLM-XComposer (浦语·灵笔).\\n'\n",
    "    '- InternLM-XComposer (浦语·灵笔) is a multi-modality conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.\\n'\n",
    "    '- InternLM-XComposer (浦语·灵笔) can understand and communicate fluently in the language chosen by the user such as English and 中文.\\n'\n",
    "    '- InternLM-XComposer (浦语·灵笔) is capable of comprehending and articulating responses effectively based on the provided image.'\n",
    "    \n",
    "    image = model.encode_img(image)\n",
    "        \n",
    "    inputs, im_mask = model.interleav_wrap_chat(model.tokenizer, query, image, [], meta_instruction)\n",
    "    # print(model.tokenizer is None) # DEBUG\n",
    "    inputs = {\n",
    "        k: v.to(model.device)\n",
    "        for k, v in inputs.items() if torch.is_tensor(v)\n",
    "    }    \n",
    "    return inputs, im_mask\n",
    "\n",
    "contrast_tokens=-8 # last {tokens} tokens are used to compute the diff in hidden_states\n",
    "alpha = 0\n",
    "layer_ids = np.arange(0, 32, 2).tolist()\n",
    "# also add end-of-assistant token in eos token id to avoid unnecessary generation\n",
    "eos_token_id = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(['[UNUSED_TOKEN_145]'])[0]\n",
    "]\n",
    "model.eval()\n",
    "\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "\n",
    "    for q in questions:\n",
    "        q_pos = pos_p + q\n",
    "        q_neg = neg_p + q\n",
    "        \n",
    "        # enc = tokenizer([input, input_pos, input_neg], return_tensors='pt', padding='longest').to(model.device)\n",
    "        inputs, im_mask = Encode(model, q, image)\n",
    "        pos_inputs, pos_im_mask = Encode(model, q_pos, image)\n",
    "        neg_inputs, neg_im_mask = Encode(model, q_neg, image)\n",
    "\n",
    "        \n",
    "        repe_args = dict(pos_input_ids=pos_inputs,\n",
    "                 pos_img_mask=pos_im_mask,\n",
    "                 neg_input_ids=neg_inputs,\n",
    "                 neg_img_mask=neg_im_mask,\n",
    "                 contrast_tokens=contrast_tokens,\n",
    "                 compute_contrast=True,\n",
    "                 use_cache=False, # not yet supporting generation with use_cache         \n",
    "                 alpha=alpha,\n",
    "                 control_layer_ids=layer_ids)\n",
    "        \n",
    "        \n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            eos_token_id=eos_token_id,\n",
    "            im_mask=im_mask,\n",
    "            do_sample=False,\n",
    "            # **repe_args,\n",
    "        )\n",
    "        \n",
    "        # @@@ controlled_outputs = model.generate(input_ids, \n",
    "        #                          attention_mask=attention_mask, \n",
    "        #                          max_new_tokens=256, \n",
    "        #                          do_sample=False, \n",
    "        #                          use_cache=False, # not yet supporting generation with use_cache\n",
    "        #                          **repe_args)\n",
    "        if image is None:\n",
    "            outputs = outputs[0].cpu().tolist()[len(inputs['input_ids'][0]):]\n",
    "        else:\n",
    "            outputs = outputs[0].cpu().tolist()\n",
    "        response = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "        print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
